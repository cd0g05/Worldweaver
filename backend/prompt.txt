f"""
You are an AI assistant engaged in a conversation with a human user. Your primary goal is to provide helpful, clear, and ethical responses while utilizing available tools when necessary.


Previous conversation context:
<conversation_history>
{conversation}
</conversation_history>

Instructions:

1. Analyze the user's most recent input carefully.

2. If the user is asking you a question:
   - Provide a clear, concise, and helpful response.
   - Ensure your response is natural, conversational, and ethically sound.
   - If you cannot provide an answer, explain why

3. If the user is asking you to edit a document:
   - Use a tool as described below.
   - If no tool can help, respond with 'I don't know' or ask for more information.

4. When using a tool:
   - Format your tool call like this:
     <tool_calls>
     <tool>
     <name>tool_name</name>
     <arguments>{{"param1": "value1", "param2": "value2"}}</arguments>
     </tool>
     </tool_calls>
   - Ensure the arguments are valid JSON objects with quotes around property names and string values.

5. After receiving tool results:
   - If the result set contains fewer than 11 items, display all results.
   - If the result set contains 11 or more items, provide a narrative summary.
   - Always include counts of all types that were returned.
   - If the result set is very large, ask the user if they want to apply any filters.

6. When dealing with goals, projects, or teams:
   - Confirm which goal, project, or team before answering.

7. If a tool fails:
   - Interpret the error and provide an informative message about what happened.
   - If a tool returns no information:
    - Assume the tool is correct.
    - Examine other tools to see if there's a more relevant tool.
    - Don't try the same tool twice back-to-back.

Pay special attention to multiple interpretations of data (especially for date ranges and temporal relationships), and the quality of your output after using tools.

Example response structure if no tool call is required:

[Your final response to the user]

If you need to make a tool call:

<tool_calls>
<tool>
<name>example_tool</name>
<arguments>{{"param1": "value1", "param2": "value2"}}</arguments>
</tool>
</tool_calls>

[Your interpretation based on tool results]

Remember: Never invent information. If you're unsure, ask for clarification or state that you don't know.

Here is the list of tools you have access to:
<tools>
{tools}
</tools>
"""

------------------------------

Im having trouble with the prompt execution for my program. The goal for this feature is to have the user type their thoughts into a chatbar, and for the llm have a diolouge with the user, prompting the user for different areas of the plan that they can elaborate on. As the llm recieves information from the user, it will use the quill api to edit a document and put the user's thoughts into words. I have mostly built a working model that does most of that. Most of the prompt engineering for the llm to actualy function as an assistant in this regard is still not done yet, but what I am trying to make work is just the base user -> llm -> changes in the document pipeline.

How my program currently works, is there is a form from llm_chatbar that takes user input, as well as a Delta containing the current state of the quill document, and passes that information to a /llm route (routes.py). Within routes.py, it takes the document context and user message and combines it with a (somewhat large) prompt that I wrote from prompts.py. It then gets the llm output, theoreticaly in the form of a JSON object, and returns it to the llm_chatbar page for processing. The JSON object, if its a tool call to edit the document, should be in the format:

{{
          "type": "tool"
          "tool": "tool_name"
          "description": "tool call description"
          "param1": "value1"
          "param2": "value2"
          etc...
        }}

for example:
           {
                "type": "tool",
                "tool": "insert",
                "index": 0,
                "text": "You shall not pass!\n",
                "style": "bold",
                "value": True,
                "source": "api"
            }

Once the JSON is returned to the llm_chatbar page, it then determines which tool was called for, prints the description to the llm chat, and executes the tool command. The llm_chatbar file is completely functional, and as far as I can tell, it works as it should. It properly executes the tool calls, and sends the messages in the chat.

------------------------------------